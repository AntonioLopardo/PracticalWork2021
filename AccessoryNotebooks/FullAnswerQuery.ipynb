{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from io import StringIO\n",
    "from contextlib import redirect_stdout\n",
    "from termcolor import colored\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import dataset_handler as dh\n",
    "import helper_func as hf\n",
    "\n",
    "path_parent = os.path.dirname(os.getcwd())\n",
    "os.chdir(path_parent)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "name = \"FA_gsm8k_fewer_alt_codegen_func\"\n",
    "\n",
    "file = \"data/priming_texts/gsm8k/\"+ name +\".txt\"\n",
    "\n",
    "\n",
    "with open(file, \"r\") as f:\n",
    "    prompt = f.read()\n",
    "#gptj_model = \"EleutherAI/gpt-j-6B\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "\"\"\"CodeGen runs in the venv venv\"\"\"\n",
    "model_args = hf.model_args()\n",
    "#model_args.model = \"codegen-350M-mono\"\n",
    "model, tokenizer = hf.load_CodeGen(model_args)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading parameters\n",
      "loading parameters took 499.37s\n",
      "loading tokenizer\n",
      "loading tokenizer took 4.08s\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"GPT-J and codeparrot models run in HFTest venv\"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(gptj_model)\n",
    "model = AutoModelForCausalLM.from_pretrained(gptj_model).half().eval().cuda()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "def preproc_gen_toks(gen_toks, input_len, tokenizer):\n",
    "    \"\"\"Process generated tokens from model keeping only up to \\n\\n\n",
    "\n",
    "    :param list of list gen_toks: the output form the model\n",
    "    :param int input_len: input lenght used to ignore the prompt\n",
    "    :param HF_tokenizer tokenizer: tokenizer used for decoding\n",
    "    :return list of str : list of generated outputs\n",
    "    \"\"\"\n",
    "    list_out = []\n",
    "    for gen_tok in gen_toks:\n",
    "        last_tokens = gen_tok[input_len:]\n",
    "        generated_text = tokenizer.decode(last_tokens)\n",
    "        #print(generated_text)\n",
    "        print_pattern = re.compile(r\"float\\(([^)]+)\\)\")\n",
    "        split_list = re.split(print_pattern, generated_text)\n",
    "        if len(split_list) > 1:\n",
    "            output = f\"{split_list[0]}float({split_list[1]})\\n\"\n",
    "        else:\n",
    "            output = \"INVALID OUTPUT\"\n",
    "        # output = generated_text.split(\"\\n\\n\")[0]\n",
    "        list_out.append(output)\n",
    "    return list_out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "def verify_pred_from_output(output, sample_a, print_output=False):\n",
    "        \"\"\"Verify the the output generates the solution\n",
    "\n",
    "        :param str output: output generated by the language model\n",
    "        :param str sample_a: str of solution, should be castable to float otherwise it will be changed to default wrong value\n",
    "        :return bool: True if the output generates the solution, False otherwise\n",
    "        \"\"\"\n",
    "        if print_output:\n",
    "            print(colored(f\"Return Sequence:\", \"yellow\"))\n",
    "\n",
    "        avoid_input = re.compile(r\"input\\(([^)]+)\\)\")\n",
    "        if avoid_input.search(output):\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                exec(f\"def exercise8():{output}\\n\", globals())\n",
    "                s = exercise8()\n",
    "                if print_output:\n",
    "                    print(colored(f\"{s}\", \"yellow\"))\n",
    "            except Exception as e:\n",
    "                s = 1111111111.0\n",
    "\n",
    "        is_correct = s == sample_a\n",
    "        if print_output:\n",
    "            print(colored(f\"{output}\", \"green\" if is_correct else \"red\"))\n",
    "        return is_correct\n",
    "\n",
    "verify_pred_from_output(list_outputs[2], 135, True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[33mReturn Sequence:\u001b[0m\n",
      "\u001b[33m135.0\u001b[0m\n",
      "\u001b[32m\n",
      "    sold_on_friday = 30\n",
      "    sold_on_saturday = sold_on_friday * 2\n",
      "    sold_on_sunday = sold_on_saturday - 15\n",
      "    nr_boxes = sold_on_sunday + sold_on_saturday + sold_on_friday\n",
      "    return float(nr_boxes)\n",
      "\u001b[0m\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "config = hf.codegen_gen_args()\n",
    "config.num_return_sequences = 4 # 4 for gsm8k 5 for asdiv\n",
    "config.k = 3\n",
    "config.max_lenght_after_input = 250\n",
    "config.top_p = 0.95\n",
    "config.top_k = 50\n",
    "config.temperature = 0.7\n",
    "config.min_length = 3\n",
    "\n",
    "gen_args = config"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "tokens = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "generated_tokens = model.generate(\n",
    "                tokens.long().cuda(),\n",
    "                use_cache=True,\n",
    "                do_sample=True,\n",
    "                top_k=gen_args.top_k,\n",
    "                temperature=gen_args.temperature,\n",
    "                top_p=gen_args.top_p,\n",
    "                min_length=len(tokens[0]) + gen_args.min_length,\n",
    "                max_length=len(tokens[0]) + gen_args.max_length_after_input,\n",
    "                num_return_sequences=gen_args.num_return_sequences,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "list_outputs = preproc_gen_toks(generated_tokens, len(tokens[0]), tokenizer)\n",
    "print(list_outputs[1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "    boxes_saturday = 2\n",
      "    boxes_friday = 30\n",
      "    boxes_sunday = boxes_saturday - boxes_friday\n",
      "    boxes_friday_sunday = boxes_friday + boxes_sunday\n",
      "    nr_sold_boxes = boxes_friday_sunday * 3\n",
      "    return float(nr_sold_boxes)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[33mReturn Sequence:\u001b[0m\n",
      "\u001b[31m\n",
      "    sold_on_friday = 30\n",
      "    sold_on_saturday = sold_on_friday * 2\n",
      "    sold_on_sunday = sold_on_saturday - 15\n",
      "    nr_boxes = sold_on_sunday + sold_on_saturday + sold_on_friday\n",
      "    return float(nr_boxes)\n",
      "\u001b[0m\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "exec(f\"def exercise6():{list_outputs[2]}\\n\")\n",
    "exercise6()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "135.0"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "def wrapper_exec(code):\n",
    "    exec(f\"def exercise7():{code}\\n\", globals())\n",
    "    return exercise7()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "wrapper_exec(\"return 'pleaze'\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'pleaze'"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('HFtests': venv)"
  },
  "interpreter": {
   "hash": "5ea8e8bf8ca3b9f65e7cf95ea2223aa98d68a16922d567d33b86408fe49c3092"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}