{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "cur_dir = os.getcwd()\n",
    "os.chdir(os.path.join(cur_dir, 'data'))\n",
    "!git clone https://gitlab.cs.washington.edu/ALGES/TACL2015.git\n",
    "!git clone https://github.com/chaochun/nlu-asdiv-dataset.git\n",
    "!git clone https://github.com/openai/grade-school-math.git\n",
    "os.chdir(cur_dir)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'TACL2015'...\n",
      "remote: Enumerating objects: 2294, done.\u001b[K\n",
      "remote: Counting objects: 100% (2294/2294), done.\u001b[K\n",
      "remote: Compressing objects: 100% (2234/2234), done.\u001b[K\n",
      "remote: Total 2294 (delta 203), reused 2103 (delta 55), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (2294/2294), 4.51 MiB | 3.62 MiB/s, done.\n",
      "Resolving deltas: 100% (203/203), done.\n",
      "Cloning into 'nlu-asdiv-dataset'...\n",
      "remote: Enumerating objects: 30, done.\u001b[K\n",
      "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
      "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
      "remote: Total 30 (delta 6), reused 20 (delta 5), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (30/30), 425.56 KiB | 1.67 MiB/s, done.\n",
      "Cloning into 'grade-school-math'...\n",
      "remote: Enumerating objects: 36, done.\u001b[K\n",
      "remote: Counting objects: 100% (36/36), done.\u001b[K\n",
      "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
      "remote: Total 36 (delta 14), reused 30 (delta 11), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (36/36), 3.01 MiB | 4.94 MiB/s, done.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "from xml.etree import ElementTree\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from io import StringIO\n",
    "from contextlib import redirect_stdout\n",
    "from termcolor import colored\n",
    "import wandb"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import dataset_handler as dh\n",
    "import helper_func as hf\n",
    "import exp_impl.simple_func_def as exp_impl\n",
    "\n",
    "gptj_model = \"EleutherAI/gpt-j-6B\"\n",
    "codeparrot_model = \"lvwerra/codeparrot\"\n",
    "\n",
    "model_name = \"gpt-j\"\n",
    "#model_name = \"codegen\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "\"\"\"Load gsm8k\"\"\"\n",
    "\n",
    "if model_name == \"gpt-j\":\n",
    "    priming_text_path = (\n",
    "        \"data/priming_texts/gsm8k/gpt-j/gsm8k_fewer_alt.txt\"  # for gpt-j\n",
    "    )\n",
    "    current_dataset = dh.init_dataset_from_name(\n",
    "        \"gsm8k\", primingtext_path=priming_text_path\n",
    "    )\n",
    "else:\n",
    "    priming_text_path = \"data/priming_texts/gsm8k/codegen/gsm8k_fewer_alt_codegen_func.txt\"  # for codegen\n",
    "    current_dataset = dh.init_dataset_from_name(\n",
    "        \"gsm8k\",\n",
    "        primingtext_path=priming_text_path,\n",
    "        sample_func=exp_impl.sample_n_for_prompting,\n",
    "        generate_prompt_func=exp_impl.generate_prompt,\n",
    "    )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "\"\"\"Load asdiv\"\"\"\n",
    "\n",
    "if model_name == \"gpt-j\":\n",
    "    priming_text_path = \"data/priming_texts/asdiv/asdiv_prefix.txt\" # for gpt-j\n",
    "else:\n",
    "    priming_text_path = \"data/priming_texts/asdiv/asdiv_prefix_codegen.txt\" # for codegen\n",
    "\n",
    "current_dataset = dh.init_dataset_from_name(\"asdiv\", primingtext_path = priming_text_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hf.set_all_seeds()\n",
    "\n",
    "sample_q_list, sample_a_list = current_dataset.sample_n_for_prompting(5)\n",
    "\n",
    "print(colored(sample_q_list[0], \"blue\"))\n",
    "print(colored(sample_a_list[0], \"green\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "if model_name == \"gpt-j\":\n",
    "    \"\"\"GPT-J and codeparrot models run in HFTest venv\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(gptj_model)\n",
    "    model = AutoModelForCausalLM.from_pretrained(gptj_model).half().eval().cuda()\n",
    "elif model_name == \"codegen\":\n",
    "    \"\"\"CodeGen runs in the venv venv\"\"\"\n",
    "    model_args = hf.model_args()\n",
    "    #model_args.model = \"codegen-350M-mono\"\n",
    "    model, tokenizer = hf.load_CodeGen(model_args)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Set up for CodeGen\n",
    "config = hf.codegen_gen_args()\n",
    "config.num_return_sequences = 4 # 4 for gsm8k 5 for asdiv\n",
    "config.k = 3\n",
    "config.max_lenght_after_input = 250\n",
    "config.top_p = 0.95\n",
    "config.top_k = 50\n",
    "config.temperature = 0.7\n",
    "config.min_length = 3\n",
    "\n",
    "hf.set_all_seeds(model_name)\n",
    "hf.testing_loop(current_dataset, tokenizer, model, sample_q_list, sample_a_list, config, func_def_mod=True, print_output=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Set up for gpt-j\n",
    "config = hf.gptj_gen_args()\n",
    "\n",
    "hf.set_all_seeds(model_name)\n",
    "hf.testing_loop(current_dataset, tokenizer, model, sample_q_list, sample_a_list, config, print_output=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with wandb.init(project=\"PracticalWork\", entity=\"antoniolopardo\",config=config, name=\"@100-gsm8k-codegen-func-def\"):\n",
    "\n",
    "        hf.set_all_seeds(model)\n",
    "        pass_at_k = hf.testing_loop(current_dataset, tokenizer, model, sample_q_list, sample_a_list, config)\n",
    "\n",
    "        wandb.log({\"pass_at_k\": pass_at_k})"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('HFtests': venv)"
  },
  "interpreter": {
   "hash": "5ea8e8bf8ca3b9f65e7cf95ea2223aa98d68a16922d567d33b86408fe49c3092"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}