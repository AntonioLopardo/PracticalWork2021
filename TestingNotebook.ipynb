{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "cur_dir = os.getcwd()\n",
    "os.chdir(os.path.join(cur_dir, 'data'))\n",
    "!git clone https://gitlab.cs.washington.edu/ALGES/TACL2015.git\n",
    "!git clone https://github.com/chaochun/nlu-asdiv-dataset.git\n",
    "!git clone https://github.com/openai/grade-school-math.git\n",
    "os.chdir(cur_dir)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "fatal: destination path 'TACL2015' already exists and is not an empty directory.\n",
      "fatal: destination path 'nlu-asdiv-dataset' already exists and is not an empty directory.\n",
      "fatal: destination path 'grade-school-math' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "from xml.etree import ElementTree\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from io import StringIO\n",
    "from contextlib import redirect_stdout\n",
    "from termcolor import colored"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import dataset_handler as dh\n",
    "import helper_func as hf"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(8)\n",
    "\n",
    "gptj_model = \"EleutherAI/gpt-j-6B\"\n",
    "codeparrot_model = \"lvwerra/codeparrot\"\n",
    "\n",
    "\"\"\"Load the priming text to add to the prompt and sample a question\"\"\"\n",
    "priming_text_path = \"data/priming_texts/gsm8k/gsm8k_fewer_alt_codegen.txt\"\n",
    "# priming_text = read_string_from_file(\"data/priming_texts/singleEq.txt\")\n",
    "# priming_text_path = \"data/priming_texts/asdiv/asdiv_prefix_codegen.txt\"\n",
    "\n",
    "current_dataset = dh.init_dataset_from_name(\"gsm8k\", primingtext_path = priming_text_path)\n",
    "\n",
    "sample_q_list, sample_a_list = current_dataset.sample_n_for_prompting(10)\n",
    "\n",
    "print(colored(sample_q_list[0], \"blue\"))\n",
    "print(colored(sample_a_list[0], \"green\"))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[34mWrite a program that prints the answer to the following question. Carol sells tickets for an exhibition. During three days she sold tickets worth $960. One ticket costs $4. How many tickets on average did she sell during one of these three days?\u001b[0m\n",
      "\u001b[32m80\u001b[0m\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "\"\"\"CodeGen runs in the venv venv\"\"\"\n",
    "model, tokenizer = hf.load_CodeGen()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading parameters\n",
      "loading parameters took 432.60s\n",
      "loading tokenizer\n",
      "loading tokenizer took 4.17s\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "\"\"\"GPT-J and codeparrot models run in HFTest venv\"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(gptj_model)\n",
    "model = AutoModelForCausalLM.from_pretrained(gptj_model).half().eval().cuda()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "n = 3\n",
    "k = 3\n",
    "\"\"\"n = 3\n",
    "k = 3\"\"\"\n",
    "\n",
    "hf.testing_loop(n, k, current_dataset, tokenizer, model, sample_q_list, sample_a_list)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[33mTESTING STARTED\u001b[0m\n",
      "\u001b[37m@sample 5 -> Pass@3 = 0.2\u001b[0m\n",
      "\u001b[37m@sample 10 -> Pass@3 = 0.2\u001b[0m\n",
      "\u001b[32m\n",
      "\n",
      "Pass@3 = 0.2\u001b[0m\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "64d063b1faafbf824c011538bbdc5dd21a33fbfbbf626b1c4ab6e2989961df88"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}