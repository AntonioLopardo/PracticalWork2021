{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from io import StringIO\n",
    "from contextlib import redirect_stdout\n",
    "from termcolor import colored\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import dataset_handler as dh\n",
    "import loading_utils as lu\n",
    "import testing_utils as tu\n",
    "\n",
    "#path_parent = os.path.dirname(os.getcwd())\n",
    "#os.chdir(path_parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.16.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"test_func_full_answer\"\n",
    "\n",
    "file = \"data/priming_texts/gsm8k/codegen/test/\"+ name +\".txt\"\n",
    "\n",
    "with open(file, \"r\") as f:\n",
    "    prompt = f.read()\n",
    "#gptj_model = \"EleutherAI/gpt-j-6B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading parameters\n",
      "loading parameters took 462.04s\n",
      "loading tokenizer\n",
      "loading tokenizer took 4.01s\n"
     ]
    }
   ],
   "source": [
    "\"\"\"CodeGen runs in the venv venv\"\"\"\n",
    "model_args = lu.model_args()\n",
    "#model_args.model = \"codegen-350M-mono\"\n",
    "model, tokenizer = lu.load_CodeGen(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"GPT-J and codeparrot models run in HFTest venv\"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(gptj_model)\n",
    "model = AutoModelForCausalLM.from_pretrained(gptj_model).half().eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_gen_toks(gen_toks, input_len, tokenizer, save_seqs=False):\n",
    "    \"\"\"Process generated tokens from model keeping only up to \\n\\n\n",
    "\n",
    "    :param list of list gen_toks: the output form the model\n",
    "    :param int input_len: input lenght used to ignore the prompt\n",
    "    :param HF_tokenizer tokenizer: tokenizer used for decoding\n",
    "    :return list of str : list of generated outputs\n",
    "    \"\"\"\n",
    "    list_out = []\n",
    "    for gen_tok in gen_toks:\n",
    "        last_tokens = gen_tok[input_len:]\n",
    "        if save_seqs:\n",
    "            full_seq = tokenizer.decode(gen_tok)\n",
    "            with open(\"sample_seqs.txt\", \"w\") as f:\n",
    "                f.write(full_seq)\n",
    "        generated_text = tokenizer.decode(last_tokens)\n",
    "        #print(generated_text)\n",
    "        print_pattern = re.compile(r\"float\\(([^)]+)\\)\")\n",
    "        split_list = re.split(print_pattern, generated_text)\n",
    "        if len(split_list) > 1:\n",
    "            output = f\"{split_list[0]}float({split_list[1]})\\n\"\n",
    "        else:\n",
    "            output = \"INVALID OUTPUT\"\n",
    "    \n",
    "        list_out.append(output)\n",
    "    return list_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_pred_from_output(output, sample_a, print_output=False):\n",
    "        \"\"\"Verify the the output generates the solution\n",
    "\n",
    "        :param str output: output generated by the language model\n",
    "        :param str sample_a: str of solution, should be castable to float otherwise it will be changed to default wrong value\n",
    "        :return bool: True if the output generates the solution, False otherwise\n",
    "        \"\"\"\n",
    "        if print_output:\n",
    "            print(colored(f\"Return Sequence:\", \"yellow\"))\n",
    "\n",
    "        avoid_input = re.compile(r\"input\\(([^)]+)\\)\")\n",
    "        if avoid_input.search(output):\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                exec(f\"def exercise6():{output}\\n\", globals())\n",
    "                s = exercise6()\n",
    "                if print_output:\n",
    "                    print(colored(f\"{s}\", \"yellow\"))\n",
    "            except Exception as e:\n",
    "                s = 1111111111.0\n",
    "\n",
    "        is_correct = s == sample_a\n",
    "        if print_output:\n",
    "            print(colored(f\"{output}\", \"green\" if is_correct else \"red\"))\n",
    "        return is_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = lu.codegen_gen_args()\n",
    "#config.num_return_sequences = 4 # 4 for gsm8k 5 for asdiv\n",
    "config.num_return_sequences = 6\n",
    "config.k = 3\n",
    "config.max_lenght_after_input = 250\n",
    "#config.top_p = 0.95\n",
    "config.top_p = 0.95\n",
    "config.top_k = 50\n",
    "#config.temperature = 0.7\n",
    "config.temperature = 0.61\n",
    "config.min_length = 3\n",
    "\n",
    "gen_args = config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    boxes_saturday = 30\n",
      "    boxes_sunday = boxes_saturday * 2\n",
      "    boxes_friday = boxes_saturday * 2\n",
      "    boxes_total = boxes_saturday + boxes_sunday + boxes_friday\n",
      "    return float(boxes_total)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "generated_tokens = model.generate(\n",
    "                tokens.long().cuda(),\n",
    "                use_cache=True,\n",
    "                do_sample=True,\n",
    "                top_k=gen_args.top_k,\n",
    "                temperature=gen_args.temperature,\n",
    "                top_p=gen_args.top_p,\n",
    "                min_length=len(tokens[0]) + gen_args.min_length,\n",
    "                max_length=len(tokens[0]) + gen_args.max_length_after_input,\n",
    "                num_return_sequences=gen_args.num_return_sequences,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "list_outputs = preproc_gen_toks(generated_tokens, len(tokens[0]), tokenizer, save_seqs=True)\n",
    "print(list_outputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    boxes_saturday = 30\n",
      "    boxes_sunday = boxes_saturday * 2\n",
      "    boxes_friday = boxes_saturday - boxes_sunday\n",
      "    nr_total_boxes = boxes_saturday + boxes_sunday + boxes_friday\n",
      "    return float(nr_total_boxes)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(list_outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mReturn Sequence:\u001b[0m\n",
      "\u001b[33m135.0\u001b[0m\n",
      "\u001b[32m\n",
      "    boxes_sold_friday = 30\n",
      "    boxes_sold_saturday = 2 * boxes_sold_friday\n",
      "    boxes_sold_sunday = boxes_sold_saturday - 15\n",
      "    nr_boxes_sold_total = boxes_sold_friday + boxes_sold_saturday + boxes_sold_sunday\n",
      "    return float(nr_boxes_sold_total)\n",
      "\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify_pred_from_output(list_outputs[3], 135, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135.0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exec(f\"def exercise6():{list_outputs[2]}\\n\")\n",
    "exercise6()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper_exec(code):\n",
    "    exec(f\"def exercise7():{code}\\n\", globals())\n",
    "    return exercise7()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pleaze'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper_exec(\"return 'pleaze'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ea8e8bf8ca3b9f65e7cf95ea2223aa98d68a16922d567d33b86408fe49c3092"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('HFtests': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
