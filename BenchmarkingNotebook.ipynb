{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = os.getcwd()\n",
    "os.chdir(os.path.join(cur_dir, 'data'))\n",
    "!git clone https://gitlab.cs.washington.edu/ALGES/TACL2015.git\n",
    "!git clone https://github.com/chaochun/nlu-asdiv-dataset.git\n",
    "!git clone https://github.com/openai/grade-school-math.git\n",
    "os.chdir(cur_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load asdiv\"\"\"\n",
    "\n",
    "if model_name == \"gpt-j\":\n",
    "    priming_text_path = \"data/priming_texts/asdiv/asdiv_prefix.txt\" # for gpt-j\n",
    "else:\n",
    "    priming_text_path = \"data/priming_texts/asdiv/asdiv_prefix_codegen.txt\" # for codegen\n",
    "\n",
    "current_dataset = dh.init_dataset_from_name(\"asdiv\", primingtext_path = priming_text_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/CodeGen_env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from termcolor import colored\n",
    "import wandb\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/CodeGen_env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import utils.dataset_handler as dh\n",
    "import utils.loading_utils as lu\n",
    "import utils.testing_utils as tu\n",
    "\n",
    "gptj_model = \"EleutherAI/gpt-j-6B\"\n",
    "codeparrot_model = \"lvwerra/codeparrot\"\n",
    "\n",
    "#model_name = \"gpt-j\"\n",
    "model_name = \"codegen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading parameters\n",
      "loading parameters took 434.02s\n",
      "loading tokenizer\n",
      "loading tokenizer took 3.93s\n"
     ]
    }
   ],
   "source": [
    "if model_name == \"gpt-j\":\n",
    "    \"\"\"GPT-J and codeparrot models run in HFTest venv\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(gptj_model)\n",
    "    model = AutoModelForCausalLM.from_pretrained(gptj_model).half().eval().cuda()\n",
    "elif model_name == \"codegen\":\n",
    "    \"\"\"CodeGen runs in the venv venv\"\"\"\n",
    "    model_args = lu.model_args()\n",
    "    #model_args.model = \"codegen-350M-mono\"\n",
    "    model, tokenizer = lu.load_CodeGen(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mdef exercise4():\n",
      "    \"\"\"\n",
      "    Carol sells tickets for an exhibition. During three days she sold tickets worth $960. One ticket costs $4. How many tickets on average did she sell during one of these three days?\n",
      "    \"\"\"\u001b[0m\n",
      "\u001b[32m80\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "from termcolor import colored\n",
    "#import exp_impl.eq_legacy.func_def_eq_short as exp_impl\n",
    "import exp_impl.func_def_general as exp_impl\n",
    "\n",
    "\n",
    "#priming_text_path = \"data/priming_texts/gsm8k/clustering_prompt/3_clusters/cluster_2.txt\"  # for codegen\n",
    "#priming_text_path = \"data/priming_texts/gsm8k/codegen/func_eq_short.txt\"\n",
    "priming_text_path = \"data/priming_texts/gsm8k/codegen/func_short.txt\"\n",
    "#priming_text_path = \"data/priming_texts/gsm8k/concepts_prompt/part-whole_3.txt\"\n",
    "#wandb_run_name = \"@100-codegen-0\"\n",
    "importlib.reload(exp_impl)\n",
    "\n",
    "\"\"\"Load gsm8k\"\"\"\n",
    "\n",
    "if model_name == \"gpt-j\":\n",
    "    priming_text_path = (\n",
    "        \"data/priming_texts/gsm8k/gpt-j/gsm8k_fewer_alt.txt\"  # for gpt-j\n",
    "    )\n",
    "    current_dataset = dh.init_dataset_from_name(\n",
    "        \"gsm8k\", primingtext_path=priming_text_path\n",
    "    )\n",
    "else:\n",
    "    current_dataset = dh.init_dataset_from_name(\n",
    "        \"gsm8k\",\n",
    "        primingtext_path=priming_text_path,\n",
    "        sample_func=exp_impl.sample_n_for_prompting,\n",
    "        generate_prompt_func=exp_impl.generate_prompt,\n",
    "    )\n",
    "\n",
    "tu.set_all_seeds()\n",
    "#tu.set_all_seeds_alt()\n",
    "\n",
    "sample_q_list, sample_a_list = current_dataset.sample_n_for_prompting(100, inc_eq=False)\n",
    "\n",
    "with open(\"test_prompt_gen.txt\", \"w\") as f:\n",
    "    f.write(current_dataset.generate_prompt(sample_q_list[0]))\n",
    "\n",
    "print(colored(sample_q_list[0], \"blue\"))\n",
    "print(colored(sample_a_list[0], \"green\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mTESTING STARTED\u001b[0m\n",
      "\u001b[37m@sample 5 -> Pass@3 = 0.35\u001b[0m\n",
      "\u001b[37m@sample 10 -> Pass@3 = 0.275\u001b[0m\n",
      "\u001b[37m@sample 15 -> Pass@3 = 0.18333333333333332\u001b[0m\n",
      "\u001b[37m@sample 20 -> Pass@3 = 0.175\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/PracticalWork2021/BenchmarkingNotebook.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B65.108.33.73/home/PracticalWork2021/BenchmarkingNotebook.ipynb#ch0000009vscode-remote?line=11'>12</a>\u001b[0m config\u001b[39m.\u001b[39mmin_length \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B65.108.33.73/home/PracticalWork2021/BenchmarkingNotebook.ipynb#ch0000009vscode-remote?line=13'>14</a>\u001b[0m tu\u001b[39m.\u001b[39mset_all_seeds(model_name)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B65.108.33.73/home/PracticalWork2021/BenchmarkingNotebook.ipynb#ch0000009vscode-remote?line=14'>15</a>\u001b[0m _, pass_at_k_list \u001b[39m=\u001b[39m tu\u001b[39m.\u001b[39;49mtesting_loop(current_dataset, tokenizer, model, sample_q_list, sample_a_list, config, func_def_mod\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, print_output\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m/home/PracticalWork2021/testing_utils.py:156\u001b[0m, in \u001b[0;36mtesting_loop\u001b[0;34m(current_dataset, tokenizer, model, sample_q_list, sample_a_list, gen_args, func_def_mod, print_output)\u001b[0m\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/testing_utils.py?line=153'>154</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/testing_utils.py?line=154'>155</a>\u001b[0m     tokens \u001b[39m=\u001b[39m tokenizer(prompt, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39minput_ids\n\u001b[0;32m--> <a href='file:///home/PracticalWork2021/testing_utils.py?line=155'>156</a>\u001b[0m     generated_tokens \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/testing_utils.py?line=156'>157</a>\u001b[0m         tokens\u001b[39m.\u001b[39;49mlong()\u001b[39m.\u001b[39;49mcuda(),\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/testing_utils.py?line=157'>158</a>\u001b[0m         use_cache\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/testing_utils.py?line=158'>159</a>\u001b[0m         do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/testing_utils.py?line=159'>160</a>\u001b[0m         top_k\u001b[39m=\u001b[39;49mgen_args\u001b[39m.\u001b[39;49mtop_k,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/testing_utils.py?line=160'>161</a>\u001b[0m         temperature\u001b[39m=\u001b[39;49mgen_args\u001b[39m.\u001b[39;49mtemperature,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/testing_utils.py?line=161'>162</a>\u001b[0m         top_p\u001b[39m=\u001b[39;49mgen_args\u001b[39m.\u001b[39;49mtop_p,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/testing_utils.py?line=162'>163</a>\u001b[0m         min_length\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(tokens[\u001b[39m0\u001b[39;49m]) \u001b[39m+\u001b[39;49m gen_args\u001b[39m.\u001b[39;49mmin_length,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/testing_utils.py?line=163'>164</a>\u001b[0m         max_length\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(tokens[\u001b[39m0\u001b[39;49m]) \u001b[39m+\u001b[39;49m gen_args\u001b[39m.\u001b[39;49mmax_length_after_input,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/testing_utils.py?line=164'>165</a>\u001b[0m         num_return_sequences\u001b[39m=\u001b[39;49mgen_args\u001b[39m.\u001b[39;49mnum_return_sequences,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/testing_utils.py?line=165'>166</a>\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/testing_utils.py?line=166'>167</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/testing_utils.py?line=168'>169</a>\u001b[0m     list_outputs \u001b[39m=\u001b[39m preproc_gen_toks(\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/testing_utils.py?line=169'>170</a>\u001b[0m         generated_tokens, \u001b[39mlen\u001b[39m(tokens[\u001b[39m0\u001b[39m]), tokenizer, func_def_mod\u001b[39m=\u001b[39mfunc_def_mod\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/testing_utils.py?line=170'>171</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/testing_utils.py?line=172'>173</a>\u001b[0m     \u001b[39mif\u001b[39;00m print_output:\n",
      "File \u001b[0;32m/home/CodeGen_env/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[0;32m---> <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=27'>28</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/home/CodeGen_env/lib/python3.8/site-packages/transformers/generation_utils.py:1200\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/transformers/generation_utils.py?line=1191'>1192</a>\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/transformers/generation_utils.py?line=1192'>1193</a>\u001b[0m         input_ids,\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/transformers/generation_utils.py?line=1193'>1194</a>\u001b[0m         expand_size\u001b[39m=\u001b[39mnum_return_sequences,\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/transformers/generation_utils.py?line=1194'>1195</a>\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/transformers/generation_utils.py?line=1195'>1196</a>\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/transformers/generation_utils.py?line=1196'>1197</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/transformers/generation_utils.py?line=1198'>1199</a>\u001b[0m     \u001b[39m# 12. run sample\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/transformers/generation_utils.py?line=1199'>1200</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/transformers/generation_utils.py?line=1200'>1201</a>\u001b[0m         input_ids,\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/transformers/generation_utils.py?line=1201'>1202</a>\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/transformers/generation_utils.py?line=1202'>1203</a>\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/transformers/generation_utils.py?line=1203'>1204</a>\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/transformers/generation_utils.py?line=1204'>1205</a>\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mpad_token_id,\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/transformers/generation_utils.py?line=1205'>1206</a>\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49meos_token_id,\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/transformers/generation_utils.py?line=1206'>1207</a>\u001b[0m         output_scores\u001b[39m=\u001b[39;49moutput_scores,\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/transformers/generation_utils.py?line=1207'>1208</a>\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/transformers/generation_utils.py?line=1208'>1209</a>\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/transformers/generation_utils.py?line=1209'>1210</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/transformers/generation_utils.py?line=1210'>1211</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/transformers/generation_utils.py?line=1212'>1213</a>\u001b[0m \u001b[39melif\u001b[39;00m is_beam_gen_mode:\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/transformers/generation_utils.py?line=1213'>1214</a>\u001b[0m     \u001b[39mif\u001b[39;00m num_return_sequences \u001b[39m>\u001b[39m num_beams:\n",
      "File \u001b[0;32m/home/CodeGen_env/lib/python3.8/site-packages/transformers/generation_utils.py:1710\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/transformers/generation_utils.py?line=1706'>1707</a>\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/transformers/generation_utils.py?line=1708'>1709</a>\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/transformers/generation_utils.py?line=1709'>1710</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/transformers/generation_utils.py?line=1710'>1711</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/transformers/generation_utils.py?line=1711'>1712</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/transformers/generation_utils.py?line=1712'>1713</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/transformers/generation_utils.py?line=1713'>1714</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/transformers/generation_utils.py?line=1714'>1715</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/transformers/generation_utils.py?line=1716'>1717</a>\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/transformers/generation_utils.py?line=1717'>1718</a>\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/home/CodeGen_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py:628\u001b[0m, in \u001b[0;36mCodeGenForCausalLM.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=619'>620</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=620'>621</a>\u001b[0m \u001b[39mlabels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=621'>622</a>\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=622'>623</a>\u001b[0m \u001b[39m    ``labels = input_ids`` Indices are selected in ``[-100, 0, ..., config.vocab_size]`` All labels set to\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=623'>624</a>\u001b[0m \u001b[39m    ``-100`` are ignored (masked), the loss is only computed for labels in ``[0, ..., config.vocab_size]``\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=624'>625</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=625'>626</a>\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=627'>628</a>\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=628'>629</a>\u001b[0m     input_ids,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=629'>630</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=630'>631</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=631'>632</a>\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=632'>633</a>\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=633'>634</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=634'>635</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=635'>636</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=636'>637</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=637'>638</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=638'>639</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=639'>640</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=640'>641</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=642'>643</a>\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/home/CodeGen_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py:500\u001b[0m, in \u001b[0;36mCodeGenModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=491'>492</a>\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=492'>493</a>\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=493'>494</a>\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=496'>497</a>\u001b[0m         head_mask[i],\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=497'>498</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=498'>499</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=499'>500</a>\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=500'>501</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=501'>502</a>\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=502'>503</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=503'>504</a>\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=504'>505</a>\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=505'>506</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=506'>507</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=508'>509</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=509'>510</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/home/CodeGen_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py:262\u001b[0m, in \u001b[0;36mCodeGenBlock.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=259'>260</a>\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=260'>261</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=261'>262</a>\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=262'>263</a>\u001b[0m     hidden_states,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=263'>264</a>\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=264'>265</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=265'>266</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=266'>267</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=267'>268</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=268'>269</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=269'>270</a>\u001b[0m attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=270'>271</a>\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m/home/CodeGen_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/CodeGen_env/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py:184\u001b[0m, in \u001b[0;36mCodeGenAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_past, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=180'>181</a>\u001b[0m q_rot \u001b[39m=\u001b[39m query[:, :, :, : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrotary_dim]\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=181'>182</a>\u001b[0m q_pass \u001b[39m=\u001b[39m query[:, :, :, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrotary_dim :]\n\u001b[0;32m--> <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=183'>184</a>\u001b[0m sincos \u001b[39m=\u001b[39m fixed_pos_embedding(k_rot, \u001b[39m1\u001b[39;49m, seq_len\u001b[39m=\u001b[39;49mseq_len)\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=184'>185</a>\u001b[0m k_rot \u001b[39m=\u001b[39m apply_rotary_pos_emb(k_rot, sincos, offset\u001b[39m=\u001b[39moffset)\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=185'>186</a>\u001b[0m q_rot \u001b[39m=\u001b[39m apply_rotary_pos_emb(q_rot, sincos, offset\u001b[39m=\u001b[39moffset)\n",
      "File \u001b[0;32m/home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py:43\u001b[0m, in \u001b[0;36mfixed_pos_embedding\u001b[0;34m(x, seq_dim, seq_len)\u001b[0m\n\u001b[1;32m     <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=40'>41</a>\u001b[0m     seq_len \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape[seq_dim]\n\u001b[1;32m     <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=41'>42</a>\u001b[0m inv_freq \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m (\u001b[39m10000\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m (torch\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m, dim, \u001b[39m2\u001b[39m) \u001b[39m/\u001b[39m dim))\n\u001b[0;32m---> <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=42'>43</a>\u001b[0m sinusoid_inp \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49meinsum(\u001b[39m\"\u001b[39;49m\u001b[39mi , j -> i j\u001b[39;49m\u001b[39m\"\u001b[39;49m, torch\u001b[39m.\u001b[39;49marange(seq_len), inv_freq)\u001b[39m.\u001b[39;49mto(x\u001b[39m.\u001b[39;49mdevice)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m     <a href='file:///home/PracticalWork2021/CodeGen/jaxformer/hf/codegen/modeling_codegen.py?line=43'>44</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39msin(sinusoid_inp), torch\u001b[39m.\u001b[39mcos(sinusoid_inp)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set up for CodeGen\n",
    "config = lu.codegen_gen_args()\n",
    "#config.num_return_sequences = 4 # 4 for gsm8k 5 for asdiv\n",
    "config.num_return_sequences = 4\n",
    "config.k = 3\n",
    "config.max_lenght_after_input = 250\n",
    "#config.top_p = 0.95\n",
    "config.top_p = 0.95\n",
    "config.top_k = 50\n",
    "#config.temperature = 0.7\n",
    "config.temperature = 0.61\n",
    "config.min_length = 3\n",
    "\n",
    "tu.set_all_seeds(model_name)\n",
    "_, pass_at_k_list = tu.testing_loop(current_dataset, tokenizer, model, sample_q_list, sample_a_list, config, func_def_mod=True, print_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"results_lists/{priming_text_path}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pass_at_k_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mTransfer prompts - Pass@3 = 0.32899999999999996\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(colored(f\"Transfer prompts - Pass@{3} = {np.mean(np.array(general_pass_at_k))}\", \"green\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mTransfer prompts - Pass@3 = 0.36399999999999993\u001b[0m\n",
      "\u001b[32mTransfer prompts - Pass@3 = 0.338\u001b[0m\n",
      "\u001b[32mTransfer prompts - Pass@3 = 0.391\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(colored(f\"Transfer prompts - Pass@{3} = {np.mean(np.array(cluster_0_pass_at_k))}\", \"green\"))\n",
    "print(colored(f\"Transfer prompts - Pass@{3} = {np.mean(np.array(cluster_1_pass_at_k))}\", \"green\"))\n",
    "print(colored(f\"Transfer prompts - Pass@{3} = {np.mean(np.array(cluster_2_pass_at_k))}\", \"green\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"clustering_exp/3_cluster/cluster_0_pass_at_k.pkl\", \"wb\") as f:\n",
    "    pickle.dump(cluster_0_pass_at_k, f)\n",
    "with open(\"clustering_exp/3_cluster/cluster_1_pass_at_k.pkl\", \"wb\") as f:\n",
    "    pickle.dump(cluster_1_pass_at_k, f)\n",
    "with open(\"clustering_exp/3_cluster/cluster_2_pass_at_k.pkl\", \"wb\") as f:\n",
    "    pickle.dump(cluster_2_pass_at_k, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'part_whole_pass_at_k' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/PracticalWork2021/BenchmarkingNotebook.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B65.108.33.99/home/PracticalWork2021/BenchmarkingNotebook.ipynb#ch0000017vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(colored(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPart-whole promts - Pass@\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m3\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m = \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mmean(np\u001b[39m.\u001b[39marray(part_whole_pass_at_k))\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgreen\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'part_whole_pass_at_k' is not defined"
     ]
    }
   ],
   "source": [
    "print(colored(f\"Part-whole promts - Pass@{3} = {np.mean(np.array(part_whole_pass_at_k))}\", \"green\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mTransfer prompts - Pass@3 = 0.345\u001b[0m\n",
      "\u001b[32mDimansional analysis promts - Pass@3 = 0.318\u001b[0m\n",
      "\u001b[32mExplicit math promts - Pass@3 = 0.37799999999999995\u001b[0m\n",
      "\u001b[32mPart-whole promts - Pass@3 = 0.41800000000000004\u001b[0m\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'general_pass_at_k' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/PracticalWork2021/BenchmarkingNotebook.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B65.108.33.99/home/PracticalWork2021/BenchmarkingNotebook.ipynb#ch0000010vscode-remote?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(colored(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExplicit math promts - Pass@\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m3\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m = \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mmean(np\u001b[39m.\u001b[39marray(explicit_math_pass_at_k))\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgreen\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B65.108.33.99/home/PracticalWork2021/BenchmarkingNotebook.ipynb#ch0000010vscode-remote?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(colored(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPart-whole promts - Pass@\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m3\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m = \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mmean(np\u001b[39m.\u001b[39marray(part_whole_pass_at_k))\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgreen\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B65.108.33.99/home/PracticalWork2021/BenchmarkingNotebook.ipynb#ch0000010vscode-remote?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(colored(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGeneral prompts - Pass@\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m3\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m = \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mmean(np\u001b[39m.\u001b[39marray(general_pass_at_k))\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgreen\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'general_pass_at_k' is not defined"
     ]
    }
   ],
   "source": [
    "print(colored(f\"Transfer prompts - Pass@{3} = {np.mean(np.array(transfer_pass_at_k))}\", \"green\"))\n",
    "print(colored(f\"Dimansional analysis promts - Pass@{3} = {np.mean(np.array(dimension_analysis_pass_at_k))}\", \"green\"))\n",
    "print(colored(f\"Explicit math promts - Pass@{3} = {np.mean(np.array(explicit_math_pass_at_k))}\", \"green\"))\n",
    "print(colored(f\"Part-whole promts - Pass@{3} = {np.mean(np.array(part_whole_pass_at_k))}\", \"green\"))\n",
    "print(colored(f\"General prompts - Pass@{3} = {np.mean(np.array(general_pass_at_k))}\", \"green\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"transfer_pass_at_k.pkl\", \"wb\") as f:\n",
    "    pickle.dump(transfer_pass_at_k, f)\n",
    "with open(\"dimension_analysis_pass_at_k.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dimension_analysis_pass_at_k, f)\n",
    "with open(\"explicit_math_pass_at_k.pkl\", \"wb\") as f:\n",
    "    pickle.dump(explicit_math_pass_at_k, f)\n",
    "with open(\"part_whole_pass_at_k.pkl\", \"wb\") as f:\n",
    "    pickle.dump(part_whole_pass_at_k, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up for gpt-j\n",
    "#config = lu.gptj_gen_args()\n",
    "\n",
    "#tu.set_all_seeds(model_name)\n",
    "#hf.testing_loop(current_dataset, tokenizer, model, sample_q_list, sample_a_list, config, print_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/PracticalWork2021/wandb/run-20220422_152055-3qt35mwp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/antoniolopardo/PracticalWork/runs/3qt35mwp\" target=\"_blank\">@100-codegen-0</a></strong> to <a href=\"https://wandb.ai/antoniolopardo/PracticalWork\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mTESTING STARTED\u001b[0m\n",
      "\u001b[37m@sample 5 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 10 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 15 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 20 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 25 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 30 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 35 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 40 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 45 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 50 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 55 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 60 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 65 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 70 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 75 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 80 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 85 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 90 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 95 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 100 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[32m\n",
      "\n",
      "Pass@3 = 0.0\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c68ac0450d44e6a223f46692d65da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>pass_at_k</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>pass_at_k</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">@100-codegen-0</strong>: <a href=\"https://wandb.ai/antoniolopardo/PracticalWork/runs/3qt35mwp\" target=\"_blank\">https://wandb.ai/antoniolopardo/PracticalWork/runs/3qt35mwp</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220422_152055-3qt35mwp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with wandb.init(project=\"PracticalWork\", entity=\"antoniolopardo\",config=config, name=wandb_run_name):\n",
    "\n",
    "        tu.set_all_seeds(model_name)\n",
    "        pass_at_k, _ = tu.testing_loop(current_dataset, tokenizer, model, sample_q_list, sample_a_list, config, func_def_mod=True, print_output=False)\n",
    "\n",
    "        wandb.log({\"pass_at_k\": pass_at_k})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1c59954e50921301bece1313ab371cee7cd99362dbd740142445e95aebb494b1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('CodeGen_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
