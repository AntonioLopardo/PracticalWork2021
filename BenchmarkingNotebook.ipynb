{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = os.getcwd()\n",
    "os.chdir(os.path.join(cur_dir, 'data'))\n",
    "!git clone https://gitlab.cs.washington.edu/ALGES/TACL2015.git\n",
    "!git clone https://github.com/chaochun/nlu-asdiv-dataset.git\n",
    "!git clone https://github.com/openai/grade-school-math.git\n",
    "os.chdir(cur_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load asdiv\"\"\"\n",
    "\n",
    "if model_name == \"gpt-j\":\n",
    "    priming_text_path = \"data/priming_texts/asdiv/asdiv_prefix.txt\" # for gpt-j\n",
    "else:\n",
    "    priming_text_path = \"data/priming_texts/asdiv/asdiv_prefix_codegen.txt\" # for codegen\n",
    "\n",
    "current_dataset = dh.init_dataset_from_name(\"asdiv\", primingtext_path = priming_text_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from termcolor import colored\n",
    "import wandb\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_handler as dh\n",
    "import loading_utils as lu\n",
    "import testing_utils as tu\n",
    "\n",
    "gptj_model = \"EleutherAI/gpt-j-6B\"\n",
    "codeparrot_model = \"lvwerra/codeparrot\"\n",
    "\n",
    "#model_name = \"gpt-j\"\n",
    "model_name = \"codegen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'exp_impl.func_def_eq_short' from '/home/PracticalWork2021/exp_impl/func_def_eq_short.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import exp_impl.func_def_eq_short as exp_impl\n",
    "\n",
    "priming_text_path = \"data/priming_texts/gsm8k/codegen/func_eq_short.txt\"  # for codegen\n",
    "#wandb_run_name = \"@100-codegen-0\"\n",
    "importlib.reload(exp_impl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading parameters\n",
      "loading parameters took 411.89s\n",
      "loading tokenizer\n",
      "loading tokenizer took 3.87s\n"
     ]
    }
   ],
   "source": [
    "if model_name == \"gpt-j\":\n",
    "    \"\"\"GPT-J and codeparrot models run in HFTest venv\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(gptj_model)\n",
    "    model = AutoModelForCausalLM.from_pretrained(gptj_model).half().eval().cuda()\n",
    "elif model_name == \"codegen\":\n",
    "    \"\"\"CodeGen runs in the venv venv\"\"\"\n",
    "    model_args = lu.model_args()\n",
    "    #model_args.model = \"codegen-350M-mono\"\n",
    "    model, tokenizer = lu.load_CodeGen(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load gsm8k\"\"\"\n",
    "\n",
    "if model_name == \"gpt-j\":\n",
    "    priming_text_path = (\n",
    "        \"data/priming_texts/gsm8k/gpt-j/gsm8k_fewer_alt.txt\"  # for gpt-j\n",
    "    )\n",
    "    current_dataset = dh.init_dataset_from_name(\n",
    "        \"gsm8k\", primingtext_path=priming_text_path\n",
    "    )\n",
    "else:\n",
    "    current_dataset = dh.init_dataset_from_name(\n",
    "        \"gsm8k\",\n",
    "        primingtext_path=priming_text_path,\n",
    "        sample_func=exp_impl.sample_n_for_prompting,\n",
    "        generate_prompt_func=exp_impl.generate_prompt,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mdef exercise4():\n",
      "    \"\"\"Vivian plays 10 Spotify songs every day. Her best friend Clara plays 2 fewer songs each day. If in June they didn't play any song during the weekends only, and there were 8 weekend days in June, what's the total number of songs they both listened to in that month? Hint: use these equations eq1: 30-8=22 eq2: 10*22=220 eq3: 10-2=8 eq4: 8*22=176 eq5: 220+176=396\"\"\"\u001b[0m\n",
      "\u001b[32m396\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "tu.set_all_seeds()\n",
    "#tu.set_all_seeds_alt()\n",
    "\n",
    "sample_q_list, sample_a_list = current_dataset.sample_n_for_prompting(100)\n",
    "\n",
    "with open(\"test_prompt.txt\", \"w\") as f:\n",
    "    f.write(current_dataset.generate_prompt(sample_q_list[0]))\n",
    "\n",
    "print(colored(sample_q_list[3], \"blue\"))\n",
    "print(colored(sample_a_list[3], \"green\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mTESTING STARTED\u001b[0m\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/PracticalWork2021/BenchmarkingNotebook.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B65.108.33.91/home/PracticalWork2021/BenchmarkingNotebook.ipynb#ch0000009vscode-remote?line=11'>12</a>\u001b[0m config\u001b[39m.\u001b[39mmin_length \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B65.108.33.91/home/PracticalWork2021/BenchmarkingNotebook.ipynb#ch0000009vscode-remote?line=13'>14</a>\u001b[0m tu\u001b[39m.\u001b[39mset_all_seeds(model_name)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B65.108.33.91/home/PracticalWork2021/BenchmarkingNotebook.ipynb#ch0000009vscode-remote?line=14'>15</a>\u001b[0m _ \u001b[39m=\u001b[39m tu\u001b[39m.\u001b[39;49mtesting_loop(current_dataset, tokenizer, model, sample_q_list, sample_a_list, config, func_def_mod\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, print_output\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m/home/PracticalWork2021/testing_utils.py:156\u001b[0m, in \u001b[0;36mtesting_loop\u001b[0;34m(current_dataset, tokenizer, model, sample_q_list, sample_a_list, gen_args, func_def_mod, print_output)\u001b[0m\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/testing_utils.py?line=153'>154</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/testing_utils.py?line=154'>155</a>\u001b[0m     tokens \u001b[39m=\u001b[39m tokenizer(prompt, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39minput_ids\n\u001b[0;32m--> <a href='file:///home/PracticalWork2021/testing_utils.py?line=155'>156</a>\u001b[0m     generated_tokens \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/testing_utils.py?line=156'>157</a>\u001b[0m         tokens\u001b[39m.\u001b[39;49mlong()\u001b[39m.\u001b[39;49mcuda(),\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/testing_utils.py?line=157'>158</a>\u001b[0m         use_cache\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/testing_utils.py?line=158'>159</a>\u001b[0m         do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/testing_utils.py?line=159'>160</a>\u001b[0m         top_k\u001b[39m=\u001b[39;49mgen_args\u001b[39m.\u001b[39;49mtop_k,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/testing_utils.py?line=160'>161</a>\u001b[0m         temperature\u001b[39m=\u001b[39;49mgen_args\u001b[39m.\u001b[39;49mtemperature,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/testing_utils.py?line=161'>162</a>\u001b[0m         top_p\u001b[39m=\u001b[39;49mgen_args\u001b[39m.\u001b[39;49mtop_p,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/testing_utils.py?line=162'>163</a>\u001b[0m         min_length\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(tokens[\u001b[39m0\u001b[39;49m]) \u001b[39m+\u001b[39;49m gen_args\u001b[39m.\u001b[39;49mmin_length,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/testing_utils.py?line=163'>164</a>\u001b[0m         max_length\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(tokens[\u001b[39m0\u001b[39;49m]) \u001b[39m+\u001b[39;49m gen_args\u001b[39m.\u001b[39;49mmax_length_after_input,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/testing_utils.py?line=164'>165</a>\u001b[0m         num_return_sequences\u001b[39m=\u001b[39;49mgen_args\u001b[39m.\u001b[39;49mnum_return_sequences,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/testing_utils.py?line=165'>166</a>\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/testing_utils.py?line=166'>167</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/testing_utils.py?line=168'>169</a>\u001b[0m     list_outputs \u001b[39m=\u001b[39m preproc_gen_toks(\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/testing_utils.py?line=169'>170</a>\u001b[0m         generated_tokens, \u001b[39mlen\u001b[39m(tokens[\u001b[39m0\u001b[39m]), tokenizer, func_def_mod\u001b[39m=\u001b[39mfunc_def_mod\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/testing_utils.py?line=170'>171</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/PracticalWork2021/testing_utils.py?line=172'>173</a>\u001b[0m     \u001b[39mif\u001b[39;00m print_output:\n",
      "File \u001b[0;32m/home/HFtests/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/HFtests/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     <a href='file:///home/HFtests/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     <a href='file:///home/HFtests/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[0;32m---> <a href='file:///home/HFtests/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=27'>28</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/home/HFtests/lib/python3.8/site-packages/transformers/generation_utils.py:1200\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/HFtests/lib/python3.8/site-packages/transformers/generation_utils.py?line=1191'>1192</a>\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   <a href='file:///home/HFtests/lib/python3.8/site-packages/transformers/generation_utils.py?line=1192'>1193</a>\u001b[0m         input_ids,\n\u001b[1;32m   <a href='file:///home/HFtests/lib/python3.8/site-packages/transformers/generation_utils.py?line=1193'>1194</a>\u001b[0m         expand_size\u001b[39m=\u001b[39mnum_return_sequences,\n\u001b[1;32m   <a href='file:///home/HFtests/lib/python3.8/site-packages/transformers/generation_utils.py?line=1194'>1195</a>\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   <a href='file:///home/HFtests/lib/python3.8/site-packages/transformers/generation_utils.py?line=1195'>1196</a>\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   <a href='file:///home/HFtests/lib/python3.8/site-packages/transformers/generation_utils.py?line=1196'>1197</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///home/HFtests/lib/python3.8/site-packages/transformers/generation_utils.py?line=1198'>1199</a>\u001b[0m     \u001b[39m# 12. run sample\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/HFtests/lib/python3.8/site-packages/transformers/generation_utils.py?line=1199'>1200</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m   <a href='file:///home/HFtests/lib/python3.8/site-packages/transformers/generation_utils.py?line=1200'>1201</a>\u001b[0m         input_ids,\n\u001b[1;32m   <a href='file:///home/HFtests/lib/python3.8/site-packages/transformers/generation_utils.py?line=1201'>1202</a>\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   <a href='file:///home/HFtests/lib/python3.8/site-packages/transformers/generation_utils.py?line=1202'>1203</a>\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   <a href='file:///home/HFtests/lib/python3.8/site-packages/transformers/generation_utils.py?line=1203'>1204</a>\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   <a href='file:///home/HFtests/lib/python3.8/site-packages/transformers/generation_utils.py?line=1204'>1205</a>\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mpad_token_id,\n\u001b[1;32m   <a href='file:///home/HFtests/lib/python3.8/site-packages/transformers/generation_utils.py?line=1205'>1206</a>\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49meos_token_id,\n\u001b[1;32m   <a href='file:///home/HFtests/lib/python3.8/site-packages/transformers/generation_utils.py?line=1206'>1207</a>\u001b[0m         output_scores\u001b[39m=\u001b[39;49moutput_scores,\n\u001b[1;32m   <a href='file:///home/HFtests/lib/python3.8/site-packages/transformers/generation_utils.py?line=1207'>1208</a>\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   <a href='file:///home/HFtests/lib/python3.8/site-packages/transformers/generation_utils.py?line=1208'>1209</a>\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   <a href='file:///home/HFtests/lib/python3.8/site-packages/transformers/generation_utils.py?line=1209'>1210</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   <a href='file:///home/HFtests/lib/python3.8/site-packages/transformers/generation_utils.py?line=1210'>1211</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///home/HFtests/lib/python3.8/site-packages/transformers/generation_utils.py?line=1212'>1213</a>\u001b[0m \u001b[39melif\u001b[39;00m is_beam_gen_mode:\n\u001b[1;32m   <a href='file:///home/HFtests/lib/python3.8/site-packages/transformers/generation_utils.py?line=1213'>1214</a>\u001b[0m     \u001b[39mif\u001b[39;00m num_return_sequences \u001b[39m>\u001b[39m num_beams:\n",
      "File \u001b[0;32m/home/HFtests/lib/python3.8/site-packages/transformers/generation_utils.py:1747\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/HFtests/lib/python3.8/site-packages/transformers/generation_utils.py?line=1744'>1745</a>\u001b[0m \u001b[39m# sample\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/HFtests/lib/python3.8/site-packages/transformers/generation_utils.py?line=1745'>1746</a>\u001b[0m probs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(next_token_scores, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m-> <a href='file:///home/HFtests/lib/python3.8/site-packages/transformers/generation_utils.py?line=1746'>1747</a>\u001b[0m next_tokens \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmultinomial(probs, num_samples\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m   <a href='file:///home/HFtests/lib/python3.8/site-packages/transformers/generation_utils.py?line=1748'>1749</a>\u001b[0m \u001b[39m# finished sentences should have their next token be a padding token\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/HFtests/lib/python3.8/site-packages/transformers/generation_utils.py?line=1749'>1750</a>\u001b[0m \u001b[39mif\u001b[39;00m eos_token_id \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "# Set up for CodeGen\n",
    "config = lu.codegen_gen_args()\n",
    "#config.num_return_sequences = 4 # 4 for gsm8k 5 for asdiv\n",
    "config.num_return_sequences = 6\n",
    "config.k = 3\n",
    "config.max_lenght_after_input = 250\n",
    "#config.top_p = 0.95\n",
    "config.top_p = 0.95\n",
    "config.top_k = 50\n",
    "#config.temperature = 0.7\n",
    "config.temperature = 0.61\n",
    "config.min_length = 3\n",
    "\n",
    "tu.set_all_seeds(model_name)\n",
    "_, general_pass_at_k = tu.testing_loop(current_dataset, tokenizer, model, sample_q_list, sample_a_list, config, func_def_mod=True, print_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mTransfer prompts - Pass@3 = 0.3195\u001b[0m\n",
      "\u001b[32mDimansional analysis promts - Pass@3 = 0.3115\u001b[0m\n",
      "\u001b[32mExplicit math promts - Pass@3 = 0.359\u001b[0m\n",
      "\u001b[32mPart-whole promts - Pass@3 = 0.3975\u001b[0m\n",
      "\u001b[32mGeneral prompts - Pass@3 = 0.33899999999999997\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(colored(f\"Transfer prompts - Pass@{3} = {np.mean(np.array(transfer_pass_at_k))}\", \"green\"))\n",
    "print(colored(f\"Dimansional analysis promts - Pass@{3} = {np.mean(np.array(dimension_analysis_pass_at_k))}\", \"green\"))\n",
    "print(colored(f\"Explicit math promts - Pass@{3} = {np.mean(np.array(explicit_math_pass_at_k))}\", \"green\"))\n",
    "print(colored(f\"Part-whole promts - Pass@{3} = {np.mean(np.array(part_whole_pass_at_k))}\", \"green\"))\n",
    "print(colored(f\"General prompts - Pass@{3} = {np.mean(np.array(general_pass_at_k))}\", \"green\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"transfer_pass_at_k.pkl\", \"wb\") as f:\n",
    "    pickle.dump(transfer_pass_at_k, f)\n",
    "with open(\"dimension_analysis_pass_at_k.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dimension_analysis_pass_at_k, f)\n",
    "with open(\"explicit_math_pass_at_k.pkl\", \"wb\") as f:\n",
    "    pickle.dump(explicit_math_pass_at_k, f)\n",
    "with open(\"part_whole_pass_at_k.pkl\", \"wb\") as f:\n",
    "    pickle.dump(part_whole_pass_at_k, f)\n",
    "with open(\"general_pass_at_k.pkl\", \"wb\") as f:\n",
    "    pickle.dump(general_pass_at_k, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up for gpt-j\n",
    "#config = lu.gptj_gen_args()\n",
    "\n",
    "#tu.set_all_seeds(model_name)\n",
    "#hf.testing_loop(current_dataset, tokenizer, model, sample_q_list, sample_a_list, config, print_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/PracticalWork2021/wandb/run-20220422_152055-3qt35mwp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/antoniolopardo/PracticalWork/runs/3qt35mwp\" target=\"_blank\">@100-codegen-0</a></strong> to <a href=\"https://wandb.ai/antoniolopardo/PracticalWork\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mTESTING STARTED\u001b[0m\n",
      "\u001b[37m@sample 5 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 10 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 15 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 20 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 25 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 30 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 35 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 40 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 45 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 50 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 55 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 60 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 65 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 70 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 75 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 80 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 85 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 90 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 95 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[37m@sample 100 -> Pass@3 = 0.0\u001b[0m\n",
      "\u001b[32m\n",
      "\n",
      "Pass@3 = 0.0\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c68ac0450d44e6a223f46692d65da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>pass_at_k</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>pass_at_k</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">@100-codegen-0</strong>: <a href=\"https://wandb.ai/antoniolopardo/PracticalWork/runs/3qt35mwp\" target=\"_blank\">https://wandb.ai/antoniolopardo/PracticalWork/runs/3qt35mwp</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220422_152055-3qt35mwp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with wandb.init(project=\"PracticalWork\", entity=\"antoniolopardo\",config=config, name=wandb_run_name):\n",
    "\n",
    "        tu.set_all_seeds(model_name)\n",
    "        pass_at_k, _ = tu.testing_loop(current_dataset, tokenizer, model, sample_q_list, sample_a_list, config, func_def_mod=True, print_output=False)\n",
    "\n",
    "        wandb.log({\"pass_at_k\": pass_at_k})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ea8e8bf8ca3b9f65e7cf95ea2223aa98d68a16922d567d33b86408fe49c3092"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('HFtests': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
