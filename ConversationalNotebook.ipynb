{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/CodeGen_env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "from termcolor import colored\n",
    "import wandb\n",
    "import importlib\n",
    "import re\n",
    "\n",
    "import dataset_handler as dh\n",
    "import loading_utils as lu\n",
    "import testing_utils as tu\n",
    "\n",
    "gptj_model = \"EleutherAI/gpt-j-6B\"\n",
    "codeparrot_model = \"lvwerra/codeparrot\"\n",
    "\n",
    "#model_name = \"gpt-j\"\n",
    "model_name = \"codegen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'dataset_handler' from '/home/PracticalWork2021/dataset_handler.py'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(dh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading parameters\n",
      "loading parameters took 440.16s\n",
      "loading tokenizer\n",
      "loading tokenizer took 4.08s\n"
     ]
    }
   ],
   "source": [
    "transformers.set_seed(5)\n",
    "if model_name == \"gpt-j\":\n",
    "    \"\"\"GPT-J and codeparrot models run in HFTest venv\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(gptj_model)\n",
    "    model = AutoModelForCausalLM.from_pretrained(gptj_model).half().eval().cuda()\n",
    "elif model_name == \"codegen\":\n",
    "    \"\"\"CodeGen runs in the venv venv\"\"\"\n",
    "    model_args = lu.model_args()\n",
    "    #model_args.model = \"codegen-350M-mono\"\n",
    "    model, tokenizer = lu.load_CodeGen(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mdef exercise4():\n",
      "    \"\"\"\n",
      "    Carol sells tickets for an exhibition. During three days she sold tickets worth $960. One ticket costs $4. How many tickets on average did she sell during one of these three days?\n",
      "    \"\"\"\u001b[0m\n",
      "\u001b[32m80\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#import exp_impl.eq_legacy.func_def_eq_short as exp_impl\n",
    "import exp_impl.func_def_general as exp_impl\n",
    "\n",
    "\n",
    "#priming_text_path = \"data/priming_texts/gsm8k/clustering_prompt/3_clusters/cluster_2.txt\"  # for codegen\n",
    "#priming_text_path = \"data/priming_texts/gsm8k/codegen/func_eq_short.txt\"\n",
    "priming_text_path = \"data/priming_texts/gsm8k/codegen/func_short.txt\"\n",
    "#priming_text_path = \"data/priming_texts/gsm8k/concepts_prompt/part-whole_3.txt\"\n",
    "#wandb_run_name = \"@100-codegen-0\"\n",
    "importlib.reload(exp_impl)\n",
    "\n",
    "\"\"\"Load gsm8k\"\"\"\n",
    "\n",
    "if model_name == \"gpt-j\":\n",
    "    priming_text_path = (\n",
    "        \"data/priming_texts/gsm8k/gpt-j/gsm8k_fewer_alt.txt\"  # for gpt-j\n",
    "    )\n",
    "    current_dataset = dh.init_dataset_from_name(\n",
    "        \"gsm8k\", primingtext_path=priming_text_path\n",
    "    )\n",
    "else:\n",
    "    current_dataset = dh.init_dataset_from_name(\n",
    "        \"gsm8k\",\n",
    "        primingtext_path=priming_text_path,\n",
    "        sample_func=exp_impl.sample_n_for_prompting,\n",
    "        generate_prompt_func=exp_impl.generate_prompt,\n",
    "    )\n",
    "\n",
    "tu.set_all_seeds()\n",
    "#tu.set_all_seeds_alt()\n",
    "\n",
    "sample_q_list, sample_a_list = current_dataset.sample_n_for_prompting(100, inc_eq=False)\n",
    "\n",
    "with open(\"test_prompt_gen.txt\", \"w\") as f:\n",
    "    f.write(current_dataset.generate_prompt(sample_q_list[0]))\n",
    "\n",
    "print(colored(sample_q_list[0], \"blue\"))\n",
    "print(colored(sample_a_list[0], \"green\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up for CodeGen\n",
    "config = lu.codegen_gen_args()\n",
    "#config.num_return_sequences = 4 # 4 for gsm8k 5 for asdiv\n",
    "config.num_return_sequences = 1\n",
    "config.k = 3\n",
    "config.max_length_after_input = 40\n",
    "#config.top_p = 0.95\n",
    "config.top_p = 0.9\n",
    "config.top_k = 10\n",
    "#config.temperature = 0.7\n",
    "config.temperature = 1\n",
    "config.min_length = 1\n",
    "\n",
    "gen_args = config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_gen_toks(gen_toks, input_len, tokenizer):\n",
    "    #print(len(gen_toks[0]))\n",
    "    for gen_tok in gen_toks:\n",
    "        last_tokens = gen_tok[input_len:]\n",
    "        generated_text = tokenizer.decode(last_tokens)\n",
    "\n",
    "    return generated_text.split(\"\\n\")[1]\n",
    "\n",
    "from wrapt_timeout_decorator import *\n",
    "\n",
    "@timeout(45)\n",
    "def execute(c):\n",
    "    exec(c, globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.set_seed(5)\n",
    "prompt = current_dataset.generate_prompt(sample_q_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "566\n",
      "536\n",
      "566\n",
      "    return float(cars_per_level_more)\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "generated_tokens = model.generate(\n",
    "    tokens.long().cuda(),\n",
    "    use_cache=True,\n",
    "    do_sample=True,\n",
    "    top_k=gen_args.top_k,\n",
    "    temperature=gen_args.temperature,\n",
    "    top_p=gen_args.top_p,\n",
    "    min_length=len(tokens[0]) + gen_args.min_length,\n",
    "    max_length=len(tokens[0]) + gen_args.max_length_after_input,\n",
    "    num_return_sequences=gen_args.num_return_sequences,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "list_outputs = preproc_gen_toks(\n",
    "    generated_tokens, len(tokens[0]), tokenizer\n",
    ")\n",
    "print(len(tokens[0]))\n",
    "print(len(tokens[0]) + gen_args.max_length_after_input)\n",
    "print(list_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1342"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prompt += \"\\n\" + list_outputs\n",
    "#print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.0\n",
      "80.0\n",
      "7100.0\n",
      "7100.0\n",
      "name 'nr_rabbits_in_the_' is not defined\n",
      "invalid syntax (<string>, line 43)\n",
      "name 'suits_per_lily' is not defined\n",
      "name 'minutes_per_day' is not defined\n",
      "name 'tilling_length' is not defined\n",
      "unsupported operand type(s) for -: 'float' and 'list'\n",
      "73.0\n",
      "73.0\n",
      "name 'distance_between_kenn_and_' is not defined\n",
      "invalid syntax (<string>, line 42)\n",
      "1890\n",
      "1890.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.08"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.set_seed(5)\n",
    "pass_1_list = []\n",
    "\n",
    "for sample_q, sample_a in zip(sample_q_list[:50], sample_a_list[:50]):\n",
    "    prompt = current_dataset.generate_prompt(sample_q)\n",
    "\n",
    "    line_cnt = 0\n",
    "    print_pattern = re.compile(r\"def exercise4\")\n",
    "    while \"return \" not in prompt[re.search(print_pattern, prompt).start():] and line_cnt < 15:\n",
    "        line_cnt += 1\n",
    "        tokens = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "        generated_tokens = model.generate(\n",
    "            tokens.long().cuda(),\n",
    "            use_cache=True,\n",
    "            do_sample=True,\n",
    "            top_k=gen_args.top_k,\n",
    "            temperature=gen_args.temperature,\n",
    "            top_p=gen_args.top_p,\n",
    "            min_length=len(tokens[0]) + gen_args.min_length,\n",
    "            max_length=len(tokens[0]) + gen_args.max_length_after_input,\n",
    "            num_return_sequences=gen_args.num_return_sequences,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        list_outputs = preproc_gen_toks(\n",
    "            generated_tokens, len(tokens[0]), tokenizer\n",
    "        )\n",
    "        #print(len(tokens[0]))\n",
    "        #print(len(tokens[0]) + gen_args.max_length_after_input)\n",
    "        #print(list_outputs)\n",
    "\n",
    "        prompt += \"\\n\" + list_outputs\n",
    "\n",
    "\n",
    "        if \"return \" in prompt[re.search(print_pattern, prompt).start():]:\n",
    "            try:\n",
    "                #print(prompt[re.search(print_pattern, prompt).start():])\n",
    "                execute(prompt)\n",
    "                s = exercise4()\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                s = 1111111\n",
    "        #print(prompt)\n",
    "\n",
    "    pass_1_list.append(int(s == float(sample_a)))\n",
    "    if int(s == float(sample_a)) == 1:\n",
    "        print(s)\n",
    "        print(float(sample_a))\n",
    "\n",
    "np.mean(np.array(pass_1_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dataset = dh.init_dataset_from_name(\n",
    "        \"gsm8k-socratic\",\n",
    "        primingtext_path=priming_text_path,\n",
    "        sample_func=exp_impl.sample_n_for_prompting,\n",
    "        generate_prompt_func=exp_impl.generate_prompt,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How many clips did Natalia sell in May? ** Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nHow many clips did Natalia sell altogether in April and May? ** Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_dataset.data[0][\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1c59954e50921301bece1313ab371cee7cd99362dbd740142445e95aebb494b1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('CodeGen_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
